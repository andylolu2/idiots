{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from diffrax import (\n",
    "    diffeqsolve,\n",
    "    Tsit5,\n",
    "    Dopri8,\n",
    "    ODETerm,\n",
    "    SaveAt,\n",
    "    PIDController,\n",
    "    TqdmProgressMeter,\n",
    ")\n",
    "import diffrax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import neural_tangents as nt\n",
    "from einops import rearrange\n",
    "\n",
    "from idiots.dataset.image_classification import mnist_splits\n",
    "from idiots.dataset.algorithmic import binary_op_splits\n",
    "from idiots.experiments.grokking.model import EmbedMLP\n",
    "from idiots.experiments.classification.model import ImageMLP\n",
    "from idiots.experiments.classification.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: Any = get_config()\n",
    "config.train_size = 512\n",
    "config.model.init_scale = 0.3\n",
    "config.model.normalize_inputs = True\n",
    "config.model.d_model = 128\n",
    "config.model.n_layers = 2\n",
    "config.opt.lr = 1\n",
    "config.opt.weight_decay = 0\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grokking MNIST with Gradient Flow\n",
    "\n",
    "Config:\n",
    "```\n",
    "steps: 700_000\n",
    "dots_batch_size: 64\n",
    "dots_sample_size: 128\n",
    "eval_every: 1000\n",
    "log_dir: logs/checkpoints/mnist\n",
    "log_every: 100\n",
    "loss_variant: cross_entropy\n",
    "model:\n",
    "  d_model: 256\n",
    "  init_scale: 8.0\n",
    "  n_layers: 2\n",
    "  normalize_inputs: true\n",
    "opt:\n",
    "  lr: 0.001\n",
    "  name: adamw\n",
    "  warmup_steps: 10\n",
    "  weight_decay: 0.004\n",
    "save_every: -1\n",
    "seed: 0\n",
    "steps: 100000\n",
    "test_batch_size: 128\n",
    "test_size: 5000\n",
    "train_batch_size: 128\n",
    "train_size: 256\n",
    "```\n",
    "\n",
    "Grokking x + y (mod 47) GF\n",
    "\n",
    "```\n",
    "steps: 16_000_000\n",
    "dots_batch_size: 64\n",
    "dots_sample_size: 128\n",
    "eval_every: 1000\n",
    "log_dir: logs/checkpoints/mnist\n",
    "log_every: 100\n",
    "loss_variant: cross_entropy\n",
    "model:\n",
    "  d_model: 256\n",
    "  init_scale: 1.0\n",
    "  n_layers: 2\n",
    "  normalize_inputs: false\n",
    "opt:\n",
    "  learning_rate: 1\n",
    "  lr: 0.001\n",
    "  name: adamw\n",
    "  warmup_steps: 10\n",
    "  weight_decay: 0.002\n",
    "save_every: -1\n",
    "seed: 0\n",
    "steps: 100000\n",
    "test_batch_size: 128\n",
    "test_size: 5000\n",
    "train_batch_size: 128\n",
    "train_size: 128\n",
    "```\n",
    "\n",
    "Grokking x + y (mod 47) GF MSE\n",
    "\n",
    "```\n",
    "steps 50_000_000\n",
    "dots_batch_size: 64\n",
    "dots_sample_size: 128\n",
    "eval_every: 1000\n",
    "log_dir: logs/checkpoints/mnist\n",
    "log_every: 100\n",
    "loss_variant: cross_entropy\n",
    "model:\n",
    "  d_model: 256\n",
    "  init_scale: 1.0\n",
    "  n_layers: 2\n",
    "  normalize_inputs: false\n",
    "opt:\n",
    "  learning_rate: 1\n",
    "  lr: 0.001\n",
    "  name: adamw\n",
    "  warmup_steps: 10\n",
    "  weight_decay: 3.0e-05\n",
    "save_every: -1\n",
    "seed: 0\n",
    "steps: 100000\n",
    "test_batch_size: 128\n",
    "test_size: 5000\n",
    "train_batch_size: 128\n",
    "train_size: 128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = mnist_splits(config.train_size, config.test_size, config.seed)\n",
    "# ds_train, ds_test = binary_op_splits(\"x / y (mod 47)\")\n",
    "xs_train, ys_train = ds_train[\"x\"], ds_train[\"y\"]\n",
    "ys_train = 1 * jax.nn.one_hot(ys_train, ds_train.features[\"y\"].num_classes)\n",
    "xs_test, ys_test = ds_test[\"x\"], ds_test[\"y\"]\n",
    "ys_test = 1 * jax.nn.one_hot(ys_test, ds_test.features[\"y\"].num_classes)\n",
    "\n",
    "xs_train, ys_train = jax.device_put(xs_train), jax.device_put(ys_train)\n",
    "xs_test, ys_test = jax.device_put(xs_test), jax.device_put(ys_test)\n",
    "\n",
    "# xs_train = xs_train[:, [0, 2]]\n",
    "# xs_test = xs_test[:, [0, 2]]\n",
    "\n",
    "# model = TransformerSingleOutput(\n",
    "#     d_model=64,\n",
    "#     n_layers=2,\n",
    "#     n_heads=2,\n",
    "#     old_parameterisation=False,\n",
    "#     vocab_size=ds_train.features[\"y\"].num_classes,\n",
    "#     max_len=ds_train.features[\"x\"].length,\n",
    "# )\n",
    "\n",
    "model = ImageMLP(\n",
    "    hidden=config.model.d_model,\n",
    "    n_layers=config.model.n_layers,\n",
    "    normalize_inputs=config.model.normalize_inputs,\n",
    "    out=ds_train.features[\"y\"].num_classes,\n",
    ")\n",
    "\n",
    "# model = EmbedMLP(\n",
    "#     hidden=config.model.d_model,\n",
    "#     n_layers=config.model.n_layers,\n",
    "#     n_classes=ds_train.features[\"y\"].num_classes,\n",
    "# )\n",
    "params = model.init(jax.random.PRNGKey(config.seed), xs_train)\n",
    "params = jax.tree_map(lambda x: x * config.model.init_scale, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(params, xs, ys):\n",
    "    y_pred = model.apply(params, xs)\n",
    "    return jnp.mean((y_pred - ys) ** 2)\n",
    "    # return optax.softmax_cross_entropy(y_pred, ys).mean()\n",
    "\n",
    "\n",
    "def update_fn(params, xs, ys):\n",
    "    grad = jax.grad(loss_fn)(params, xs, ys)\n",
    "    update = jax.tree_map(\n",
    "        lambda g, p: -config.opt.lr * (g + config.opt.weight_decay * p),\n",
    "        grad,\n",
    "        params,\n",
    "    )\n",
    "    return update\n",
    "\n",
    "\n",
    "def fixed_norm_update_fn(params, xs, ys):\n",
    "    grad = jax.grad(loss_fn)(params, xs, ys)\n",
    "    update = jax.tree_map(\n",
    "        lambda g, p: -config.opt.lr * (g + config.opt.weight_decay * p),\n",
    "        grad,\n",
    "        params,\n",
    "    )\n",
    "\n",
    "    # project to remove the component of the update that would change the norm\n",
    "    # of the parameters\n",
    "    # u_fixed = u - (u . p^hat) p^hat\n",
    "    p_norm = optax.global_norm(params)\n",
    "    p_hat = jax.tree_map(lambda p: p / p_norm, params)\n",
    "    u_dot_p_hat = sum(jax.tree_util.tree_leaves(jax.tree_map(jnp.vdot, update, p_hat)))\n",
    "    update = jax.tree_map(lambda u, p_hat: u - u_dot_p_hat * p_hat, update, p_hat)\n",
    "    return update\n",
    "\n",
    "\n",
    "t1 = 10000\n",
    "\n",
    "term = ODETerm(lambda t, ps, args: fixed_norm_update_fn(ps, *args))\n",
    "solver = Tsit5()\n",
    "# solver = Dopri8()\n",
    "save_at = SaveAt(ts=jnp.linspace(0, t1, 101))\n",
    "step_size_controller = PIDController(rtol=1e-5, atol=1e-8, pcoeff=0.3, icoeff=0.3)\n",
    "\n",
    "sol = diffeqsolve(\n",
    "    term,\n",
    "    solver,\n",
    "    t0=0,\n",
    "    t1=t1,\n",
    "    dt0=1e-5,\n",
    "    y0=params,\n",
    "    saveat=save_at,\n",
    "    stepsize_controller=step_size_controller,\n",
    "    args=(xs_train, ys_train),\n",
    "    max_steps=None,\n",
    "    progress_meter=TqdmProgressMeter(),\n",
    ")\n",
    "sol.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def accuracy(params, xs, ys):\n",
    "    y_pred = model.apply(params, xs)\n",
    "    return jnp.mean(jnp.argmax(y_pred, axis=-1) == jnp.argmax(ys, axis=-1))\n",
    "\n",
    "\n",
    "def global_norm(params):\n",
    "    return jnp.sqrt(sum(jnp.sum(p**2) for p in jax.tree_util.tree_leaves(params)))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def dots(params, x):\n",
    "    kernel_fn = nt.batch(\n",
    "        nt.empirical_ntk_fn(\n",
    "            model.apply,\n",
    "            trace_axes=(),\n",
    "            vmap_axes=0,\n",
    "            implementation=nt.NtkImplementation.STRUCTURED_DERIVATIVES,\n",
    "        ),\n",
    "        batch_size=512,\n",
    "    )\n",
    "    k = kernel_fn(x, None, params)\n",
    "    k = rearrange(k, \"b1 b2 d1 d2 -> (b1 d1) (b2 d2)\")\n",
    "    return jnp.linalg.matrix_rank(k)  # type: ignore\n",
    "\n",
    "\n",
    "data = []\n",
    "for i, t in enumerate(sol.ts):\n",
    "    if i % 1 != 0:\n",
    "        continue\n",
    "    trained_param = jax.tree_map(lambda x: x[i], sol.ys)\n",
    "    data.append(\n",
    "        {\n",
    "            \"step\": t.item(),\n",
    "            \"weight_norm\": global_norm(trained_param).item(),\n",
    "            \"train_loss\": loss_fn(trained_param, xs_train, ys_train).item(),\n",
    "            \"train_accuracy\": accuracy(trained_param, xs_train, ys_train).item(),\n",
    "            \"test_loss\": loss_fn(trained_param, xs_test, ys_test).item(),\n",
    "            \"test_accuracy\": accuracy(trained_param, xs_test, ys_test).item(),\n",
    "            # \"dots\": dots(trained_param, xs_train[:64]).item(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_loss = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=[\"train_loss\", \"test_loss\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"loss\",\n",
    ")\n",
    "df_accuracy = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=[\"train_accuracy\", \"test_accuracy\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"accuracy\",\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "sns.lineplot(data=df_loss, x=\"step\", y=\"loss\", hue=\"split\", ax=axs[0])\n",
    "sns.lineplot(data=df_accuracy, x=\"step\", y=\"accuracy\", hue=\"split\", ax=axs[1])\n",
    "sns.lineplot(data=df, x=\"step\", y=\"weight_norm\", ax=axs[2])\n",
    "# sns.lineplot(data=df, x=\"step\", y=\"dots\", ax=axs[3])\n",
    "\n",
    "axs[0].set(yscale=\"log\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer: 1_000_000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import neural_tangents as nt\n",
    "from einops import rearrange\n",
    "\n",
    "from idiots.experiments.gradient_flow.init import restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = Path(\"logs/checkpoints/gradient_flow/exp29/checkpoints/\")\n",
    "apply_fn, init_params, ds_train, ds_test, mngr, config = restore(checkpoint_dir)\n",
    "\n",
    "xs_train, ys_train = ds_train[\"x\"], ds_train[\"y\"]\n",
    "ys_train = jax.nn.one_hot(ys_train, ds_train.features[\"y\"].num_classes)\n",
    "xs_train, ys_train = jax.device_put(xs_train), jax.device_put(ys_train)\n",
    "\n",
    "xs_test, ys_test = ds_test[\"x\"], ds_test[\"y\"]\n",
    "ys_test = jax.nn.one_hot(ys_test, ds_test.features[\"y\"].num_classes)\n",
    "xs_test, ys_test = jax.device_put(xs_test), jax.device_put(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dots(params, x):\n",
    "    kernel_fn = nt.batch(\n",
    "        nt.empirical_ntk_fn(\n",
    "            apply_fn,\n",
    "            trace_axes=(),\n",
    "            vmap_axes=0,\n",
    "            implementation=nt.NtkImplementation.STRUCTURED_DERIVATIVES,\n",
    "        ),\n",
    "        batch_size=512,\n",
    "    )\n",
    "    k = kernel_fn(x, None, params)\n",
    "    k = rearrange(k, \"b1 b2 d1 d2 -> (b1 d1) (b2 d2)\")\n",
    "    return jnp.linalg.matrix_rank(k)  # type: ignore\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def metrics(params):\n",
    "    y_pred = apply_fn(params, xs_train)\n",
    "    train_loss = jnp.mean((y_pred - ys_train) ** 2)\n",
    "    train_acc = jnp.mean(jnp.argmax(y_pred, axis=-1) == jnp.argmax(ys_train, axis=-1))\n",
    "\n",
    "    y_pred = apply_fn(params, xs_test)\n",
    "    test_loss = jnp.mean((y_pred - ys_test) ** 2)\n",
    "    test_acc = jnp.mean(jnp.argmax(y_pred, axis=-1) == jnp.argmax(ys_test, axis=-1))\n",
    "\n",
    "    weight_norm = jnp.sqrt(\n",
    "        sum(jnp.sum(p**2) for p in jax.tree_util.tree_leaves(params))\n",
    "    )\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"weight_norm\": weight_norm,\n",
    "        # \"train_dots\": dots(\n",
    "        #     params, jax.random.permutation(jax.random.PRNGKey(0), xs_train)[:128]\n",
    "        # ),\n",
    "        \"test_dots\": dots(\n",
    "            params, jax.random.permutation(jax.random.PRNGKey(0), xs_test)[:128]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "data = []\n",
    "for step in mngr.all_steps()[::5]:\n",
    "    params = mngr.restore(step)\n",
    "    train_metrics = metrics(params)\n",
    "    print(step, {k: v.item() for k, v in train_metrics.items()})\n",
    "    data.append({\"step\": step, **{k: v.item() for k, v in train_metrics.items()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df[\"step\"] += 1\n",
    "df_loss = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=df.columns[df.columns.str.contains(\"loss\")],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"loss\",\n",
    ")\n",
    "df_accuracy = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=df.columns[df.columns.str.contains(\"accuracy\")],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"accuracy\",\n",
    ")\n",
    "df_dots = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=df.columns[df.columns.str.contains(\"dots\")],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"dots\",\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "sns.lineplot(data=df_loss, x=\"step\", y=\"loss\", hue=\"split\", ax=axs[0])\n",
    "sns.lineplot(data=df_accuracy, x=\"step\", y=\"accuracy\", hue=\"split\", ax=axs[1])\n",
    "sns.lineplot(data=df, x=\"step\", y=\"weight_norm\", ax=axs[2])\n",
    "sns.lineplot(data=df_dots, x=\"step\", y=\"dots\", hue=\"split\", ax=axs[3])\n",
    "\n",
    "axs[0].set(yscale=\"log\")\n",
    "# axs[1].set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idiots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
