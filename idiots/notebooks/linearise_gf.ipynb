{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from diffrax import (\n",
    "    diffeqsolve,\n",
    "    Tsit5,\n",
    "    ODETerm,\n",
    "    SaveAt,\n",
    "    PIDController,\n",
    "    TqdmProgressMeter,\n",
    ")\n",
    "import neural_tangents as nt\n",
    "from einops import rearrange\n",
    "import scienceplots\n",
    "\n",
    "from idiots.experiments.compute_results.compute_results import restore_checkpoint\n",
    "from idiots.experiments.grokking.training import loss_fn\n",
    "\n",
    "plt.style.use([\"science\", \"grid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config, apply_fn, get_params, ds_train, ds_test, all_steps = restore_checkpoint(\n",
    "    Path(\"logs/checkpoints/gradient_flow/exp39/checkpoints\"),\n",
    "    \"gradient_flow_algorithmic\",\n",
    ")\n",
    "\n",
    "xs_train, ys_train = ds_train[\"x\"], ds_train[\"y\"]\n",
    "xs_train, ys_train = jax.device_put(xs_train), jax.device_put(ys_train)\n",
    "xs_test, ys_test = ds_test[\"x\"], ds_test[\"y\"]\n",
    "xs_test, ys_test = jax.device_put(xs_test), jax.device_put(ys_test)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearise_train_from_step(step):\n",
    "    params = get_params(step)\n",
    "    apply_fn_lin = nt.linearize(apply_fn, params)\n",
    "\n",
    "    def update_fn(params):\n",
    "        def forward(params):\n",
    "            ys_pred = apply_fn_lin(params, xs_train)\n",
    "            return loss_fn(ys_pred, ys_train, variant=config.loss_variant).mean()\n",
    "\n",
    "        grad = jax.grad(forward)(params)\n",
    "        update = jax.tree_map(\n",
    "            lambda g, p: -(g + config.weight_decay * p),\n",
    "            grad,\n",
    "            params,\n",
    "        )\n",
    "        return update\n",
    "\n",
    "    t0 = 0\n",
    "    t1 = config.T - step\n",
    "\n",
    "    term = ODETerm(lambda t, ps, args: update_fn(ps))\n",
    "    solver = Tsit5()\n",
    "    save_at = SaveAt(ts=jnp.arange(t0, t1, config.save_every))\n",
    "    step_size_controller = PIDController(\n",
    "        rtol=config.ode.rtol,\n",
    "        atol=config.ode.atol,\n",
    "        pcoeff=config.ode.pcoeff,\n",
    "        icoeff=config.ode.icoeff,\n",
    "    )\n",
    "\n",
    "    sol = diffeqsolve(\n",
    "        term,\n",
    "        solver,\n",
    "        t0=t0,\n",
    "        t1=t1,\n",
    "        dt0=None,\n",
    "        y0=get_params(step),\n",
    "        saveat=save_at,\n",
    "        stepsize_controller=step_size_controller,\n",
    "        max_steps=None,\n",
    "        progress_meter=TqdmProgressMeter(),\n",
    "    )\n",
    "    print(sol.stats)\n",
    "\n",
    "    params = [jax.tree_map(lambda x: x[i], sol.ys) for i in range(len(sol.ts))]\n",
    "\n",
    "    return apply_fn_lin, params, sol.ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn_lins = []\n",
    "params_lins = []\n",
    "ts_lins = []\n",
    "\n",
    "for step in all_steps[0:-1:10]:\n",
    "    apply_fn_lin, params_lin, ts_lin = linearise_train_from_step(step)\n",
    "    apply_fn_lins.append(apply_fn_lin)\n",
    "    params_lins.append(params_lin)\n",
    "    ts_lins.append(ts_lin + step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def accuracy(y_pred, xs, ys):\n",
    "    return jnp.mean(jnp.argmax(y_pred, axis=-1) == ys)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(y_pred, xs, ys):\n",
    "    return loss_fn(y_pred, ys, variant=config.loss_variant).mean()\n",
    "\n",
    "\n",
    "def global_norm(params):\n",
    "    return jnp.sqrt(sum(jnp.sum(p**2) for p in jax.tree_util.tree_leaves(params)))\n",
    "\n",
    "\n",
    "data = []\n",
    "for step, apply_fn_lin, params_lin, ts_lin in zip(\n",
    "    all_steps[0:-1:10], apply_fn_lins, params_lins, ts_lins\n",
    "):\n",
    "    for param, t in zip(params_lin, ts_lin):\n",
    "        y_pred_train = apply_fn_lin(param, xs_train)\n",
    "        y_pred_test = apply_fn_lin(param, xs_test)\n",
    "        data.append(\n",
    "            {\n",
    "                \"from\": step,\n",
    "                \"step\": t.item(),\n",
    "                \"weight_norm\": global_norm(param).item(),\n",
    "                \"train_loss\": loss(y_pred_train, xs_train, ys_train).item(),\n",
    "                \"train_accuracy\": accuracy(y_pred_train, xs_train, ys_train).item(),\n",
    "                \"test_loss\": loss(y_pred_test, xs_test, ys_test).item(),\n",
    "                \"test_accuracy\": accuracy(y_pred_test, xs_test, ys_test).item(),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_json(\"logs/results/linearisation.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "\n",
    "for step, params_lin, ts_lin in zip(all_steps[0:-1:10], params_lins, ts_lins):\n",
    "    checkpointer.save(\n",
    "        Path(f\"logs/linearisation/{step}\").absolute().resolve(),\n",
    "        {\n",
    "            \"step\": step,\n",
    "            \"params_lin\": params_lins,\n",
    "            \"ts\": ts_lin,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"logs/results/linearisation.json\")\n",
    "\n",
    "with open(\"logs/results/division-gf-mlp.json\", \"r\") as f:\n",
    "    df_main = pd.read_json(f)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 8 / 3))\n",
    "\n",
    "labels = []\n",
    "\n",
    "[line] = axs[0].plot(df_main[\"step\"], df_main[\"train_loss\"], color=\"C0\")\n",
    "labels.append((line, \"Train (base model)\"))\n",
    "[line] = axs[0].plot(df_main[\"step\"], df_main[\"test_loss\"], color=\"C1\")\n",
    "labels.append((line, \"Test (base model)\"))\n",
    "\n",
    "df_loss = df.melt(\n",
    "    id_vars=[\"from\", \"step\"],\n",
    "    value_vars=[\"train_loss\", \"test_loss\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"loss\",\n",
    ")\n",
    "for from_, df_ in df_loss.groupby(\"from\"):\n",
    "    for i, split in enumerate(df_[\"split\"].unique()):\n",
    "        data = df_[df_[\"split\"] == split]\n",
    "        [line] = axs[0].plot(\n",
    "            data[\"step\"], data[\"loss\"], linestyle=\"--\", alpha=0.9, color=f\"C{i}\"\n",
    "        )\n",
    "        x, y = line.get_data()\n",
    "        axs[0].plot(\n",
    "            x[0], y[0], marker=\"o\", color=line.get_color(), markersize=3, alpha=0.9\n",
    "        )\n",
    "\n",
    "labels.append((axs[0].lines[2], \"Train (linearised)\"))\n",
    "labels.append((axs[0].lines[4], \"Test (linearised)\"))\n",
    "# axs[0].legend(*zip(*labels), loc=\"upper right\")\n",
    "axs[0].set(xlabel=\"$t$\", ylabel=\"Loss\")\n",
    "\n",
    "labels = []\n",
    "[line] = axs[1].plot(df_main[\"step\"], df_main[\"training_acc\"], color=\"C0\")\n",
    "labels.append((line, \"Train (base model)\"))\n",
    "[line] = axs[1].plot(df_main[\"step\"], df_main[\"test_acc\"], color=\"C1\")\n",
    "labels.append((line, \"Test (base model)\"))\n",
    "\n",
    "df_accuracy = df.melt(\n",
    "    id_vars=[\"from\", \"step\"],\n",
    "    value_vars=[\"train_accuracy\", \"test_accuracy\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"accuracy\",\n",
    ")\n",
    "for from_, df_ in df_accuracy.groupby(\"from\"):\n",
    "    for i, split in enumerate(df_[\"split\"].unique()):\n",
    "        data = df_[df_[\"split\"] == split]\n",
    "        [line] = axs[1].plot(\n",
    "            data[\"step\"],\n",
    "            data[\"accuracy\"],\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.9,\n",
    "            color=f\"C{i}\",\n",
    "            label=f\"{from_} {split}\",\n",
    "        )\n",
    "        x, y = line.get_data()\n",
    "        axs[1].plot(\n",
    "            x[0], y[0], marker=\"o\", color=line.get_color(), markersize=3, alpha=0.9\n",
    "        )\n",
    "\n",
    "labels.append((axs[1].lines[2], \"Train (linearised)\"))\n",
    "labels.append((axs[1].lines[4], \"Test (linearised)\"))\n",
    "\n",
    "axs[1].legend(*zip(*labels), loc=\"lower right\", fontsize=\"small\")\n",
    "axs[1].set(xlabel=\"$t$\", ylabel=\"Accuracy\")\n",
    "\n",
    "# fig.savefig(\"logs/plots/linearisation.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/results/division-gf-mlp.json\", \"r\") as f:\n",
    "    df = pd.read_json(f)\n",
    "\n",
    "print(df.keys())\n",
    "df_loss = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=[\"train_loss\", \"test_loss\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"loss\",\n",
    ")\n",
    "df_accuracy = df.melt(\n",
    "    id_vars=[\"step\"],\n",
    "    value_vars=[\"training_acc\", \"test_acc\"],\n",
    "    var_name=\"split\",\n",
    "    value_name=\"accuracy\",\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "sns.lineplot(data=df_loss, x=\"step\", y=\"loss\", hue=\"split\", ax=axs[0])\n",
    "sns.lineplot(data=df_accuracy, x=\"step\", y=\"accuracy\", hue=\"split\", ax=axs[1])\n",
    "sns.lineplot(data=df, x=\"step\", y=\"weight_norm\", ax=axs[2])\n",
    "\n",
    "axs[0].set(yscale=\"log\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idiots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
