{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import neural_tangents as nt\n",
    "import orbax.checkpoint as ocp\n",
    "from absl import app, flags, logging\n",
    "from datasets import Dataset\n",
    "from ml_collections import config_flags\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import optax\n",
    "\n",
    "from idiots.dataset.dataloader import DataLoader\n",
    "from idiots.dataset.algorithmic import binary_op_splits\n",
    "from idiots.experiments.grokking.training import TrainState, dots, eval_step, train_step\n",
    "from idiots.experiments.grokking.model import TransformerSingleOutput\n",
    "from idiots.experiments.grokking.config import get_config\n",
    "from idiots.utils import metrics, num_params, get_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: Any = get_config()\n",
    "config.steps = 20000\n",
    "config.log_every = 500\n",
    "config.opt.weight_decay = 0.1\n",
    "config.model.d_model = 128\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = binary_op_splits(config.task, config.train_percentage, config.seed)\n",
    "model = TransformerSingleOutput(\n",
    "    d_model=config.model.d_model,\n",
    "    n_layers=config.model.n_layers,\n",
    "    n_heads=config.model.n_heads,\n",
    "    vocab_size=ds_train.features[\"y\"].num_classes,\n",
    "    max_len=ds_train.features[\"x\"].length,\n",
    ")\n",
    "init_params = model.init(jax.random.PRNGKey(config.seed), ds_train[\"x\"][:1])\n",
    "tx = get_optimizer(\"adamw\", **config.opt)\n",
    "state = TrainState.create(apply_fn=model.apply, params=init_params, tx=tx)\n",
    "print(f\"Model has {num_params(init_params):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(\n",
    "    DataLoader(\n",
    "        ds_train, config.train_batch_size, shuffle=True, infinite=True, drop_last=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ds, state: TrainState):\n",
    "    for batch in DataLoader(ds, config.test_batch_size):\n",
    "        logs = eval_step(state, batch, config.loss_variant)\n",
    "        metrics.log(**logs)\n",
    "    [losses, accuracies] = metrics.collect(\"eval_loss\", \"eval_accuracy\")\n",
    "    val_loss = jnp.concatenate(losses).mean().item()\n",
    "    val_acc = jnp.concatenate(accuracies).mean().item()\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while state.step < config.steps:\n",
    "    state, logs = train_step(state, next(train_iter), config.loss_variant)\n",
    "    metrics.log(**logs)\n",
    "\n",
    "    if state.step % config.log_every == 0 and config.log_every > 0:\n",
    "        [losses, accuracies] = metrics.collect(\"loss\", \"accuracy\")\n",
    "        train_loss = jnp.concatenate(losses).mean().item()\n",
    "        train_acc = jnp.concatenate(accuracies).mean().item()\n",
    "        val_loss, val_acc = evaluate(ds_test, state)\n",
    "        print(\n",
    "            f\"Step {state.step}: train/loss={train_loss:.4f} train/acc={train_acc:.4f} val/loss={val_loss:.4f} val/acc={val_acc:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jax.tree_map(lambda p: jnp.abs(p) > 0.02, state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_update(updates, params):\n",
    "    del params\n",
    "    return jax.tree_map(lambda u, m: u * m, updates, mask)\n",
    "\n",
    "\n",
    "lottery_params = jax.tree_map(lambda p, m: p * m, state.params, mask)\n",
    "new_tx = optax.chain(\n",
    "    # optax.sgd(1e-3, momentum=0.9),\n",
    "    get_optimizer(\"adamw\", **config.opt),\n",
    "    optax.stateless(mask_update),\n",
    ")\n",
    "linear_apply_fn = nt.linearize(model.apply, lottery_params)\n",
    "\n",
    "state_pruned = TrainState.create(\n",
    "    apply_fn=linear_apply_fn,\n",
    "    # apply_fn=model.apply,\n",
    "    params=lottery_params,\n",
    "    tx=new_tx,\n",
    ")\n",
    "\n",
    "evaluate(ds_test, state_pruned), evaluate(ds_train, state_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while state_pruned.step < 5000:\n",
    "    state_pruned, logs = train_step(state_pruned, next(train_iter), config.loss_variant)\n",
    "    metrics.log(**logs)\n",
    "\n",
    "    if state_pruned.step % config.log_every == 0:\n",
    "        [losses, accuracies] = metrics.collect(\"loss\", \"accuracy\")\n",
    "        train_loss = jnp.concatenate(losses).mean().item()\n",
    "        train_acc = jnp.concatenate(accuracies).mean().item()\n",
    "        val_loss, val_acc = evaluate(ds_test, state_pruned)\n",
    "        print(\n",
    "            f\"Step {state_pruned.step}: train/loss={train_loss:.4f} train/acc={train_acc:.4f} val/loss={val_loss:.4f} val/acc={val_acc:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_vectorize(params):\n",
    "    p = jax.tree_map(lambda x: jnp.abs(x).flatten(), params)\n",
    "    p = jnp.concat(jax.tree_util.tree_flatten(p)[0], axis=0)\n",
    "    return p\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"init\": magnitude_vectorize(init_params),\n",
    "        \"trained\": magnitude_vectorize(state.params),\n",
    "        \"pruned_trained\": magnitude_vectorize(state_pruned.params),\n",
    "    }\n",
    ")\n",
    "df = df.melt(var_name=\"type\", value_name=\"magnitude\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.ecdfplot(data=df, x=\"magnitude\", hue=\"type\", ax=ax, log_scale=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idiots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
