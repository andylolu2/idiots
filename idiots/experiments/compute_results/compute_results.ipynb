{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 00:41:10.875389: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-13 00:41:10.875554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-13 00:41:10.918314: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-13 00:41:13.298672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "from idiots.dataset.dataloader import DataLoader\n",
    "from idiots.experiments.grokking.training import (\n",
    "    restore as algorithmic_restore,\n",
    "    restore_partial as algorithmic_restore_partial,\n",
    "    eval_step,\n",
    ")\n",
    "from idiots.experiments.classification.training import (\n",
    "    restore as mnist_restore,\n",
    "    restore_partial as mnist_restore_partial,\n",
    ")\n",
    "from idiots.utils import metrics\n",
    "import neural_tangents as nt\n",
    "from einops import rearrange\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    StationaryKernelMixin,\n",
    "    NormalizedKernelMixin,\n",
    "    Kernel,\n",
    "    RBF,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# GP kernel object (for compatability with sklearn.gaussian_proccess)\n",
    "class CustomKernel(StationaryKernelMixin, NormalizedKernelMixin, Kernel):\n",
    "    def __init__(self, kernel_fn):\n",
    "        self.kernel_fn = kernel_fn\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, X, Y=None, eval_gradient=False):\n",
    "        kernel = np.array(self.kernel_fn(X, Y))\n",
    "\n",
    "        if eval_gradient:\n",
    "            return kernel, np.zeros(X.shape)\n",
    "        else:\n",
    "            return kernel\n",
    "\n",
    "\n",
    "# Return the transformer state and training/test accuracy/loss for each timestep\n",
    "def eval_checkpoint(\n",
    "    step,\n",
    "    batch_size,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    num_classes,\n",
    "    restore_manager,\n",
    "    restore_partial_fn,\n",
    "):\n",
    "    config, state = restore_partial_fn(\n",
    "        restore_manager, step, ds_train[\"x\"][:1], num_classes\n",
    "    )\n",
    "\n",
    "    def eval_loss_acc(ds):\n",
    "        for batch in DataLoader(ds, batch_size):\n",
    "            logs = eval_step(state, batch, config.loss_variant)\n",
    "            metrics.log(**logs)\n",
    "        [losses, accuracies] = metrics.collect(\"eval_loss\", \"eval_accuracy\")\n",
    "        loss = jnp.concatenate(losses).mean().item()\n",
    "        acc = jnp.concatenate(accuracies).mean().item()\n",
    "        return loss, acc\n",
    "\n",
    "    if len(ds_train[\"x\"]) > len(ds_test[\"x\"]):\n",
    "        ds_train = ds_train.select(range(len(ds_test[\"x\"])))\n",
    "\n",
    "    train_loss, train_acc = eval_loss_acc(ds_train)\n",
    "    test_loss, test_acc = eval_loss_acc(ds_test)\n",
    "\n",
    "    return state, train_loss, train_acc, test_loss, test_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns a dataframe representing the checkpoint data of the *transformer*, containing: \n",
    "  - step (current checkpoint step)\n",
    "  - state (of transformer network)\n",
    "  - train_loss \n",
    "  - train_acc\n",
    "  - test_loss \n",
    "  - test_acc \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_data_from_checkpoints(\n",
    "    restore_manager,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    num_classes,\n",
    "    total_steps,\n",
    "    step_distance,\n",
    "    restore_partial_fn,\n",
    "    eval_checkpoint_batch_size=512,\n",
    "):\n",
    "    data = []\n",
    "    for step in range(0, total_steps, step_distance):\n",
    "\n",
    "        print(\n",
    "            f\"Loading Data: {(step // step_distance) + 1}/{total_steps // step_distance}\"\n",
    "        )\n",
    "\n",
    "        state, train_loss, train_acc, test_loss, test_acc = eval_checkpoint(\n",
    "            step,\n",
    "            eval_checkpoint_batch_size,\n",
    "            ds_train,\n",
    "            ds_test,\n",
    "            num_classes,\n",
    "            restore_manager,\n",
    "            restore_partial_fn,\n",
    "        )\n",
    "        data.append(\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"state\": state,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Parse the general checkpoint dataframe into useful sub-dataframes and lists\n",
    "def parse_general_checkpoint_dataframe(df):\n",
    "\n",
    "    state_checkpoints = df[\"state\"].tolist()\n",
    "\n",
    "    df_loss = df[[\"step\", \"train_loss\", \"test_loss\"]]\n",
    "    df_loss = df_loss.melt(\"step\", var_name=\"split\", value_name=\"loss\")\n",
    "    df_loss[\"split\"] = df_loss[\"split\"].str.replace(\"_loss\", \"\")\n",
    "\n",
    "    df_acc = df[[\"step\", \"train_acc\", \"test_acc\"]]\n",
    "    df_acc = df_acc.melt(\"step\", var_name=\"split\", value_name=\"accuracy\")\n",
    "    df_acc[\"split\"] = df_acc[\"split\"].str.replace(\"_acc\", \"\")\n",
    "\n",
    "    steps = df[\"step\"].tolist()\n",
    "    training_loss = df_loss[df_loss[\"split\"] == \"train\"][\"loss\"].tolist()\n",
    "    test_loss = df_loss[df_loss[\"split\"] == \"test\"][\"loss\"].tolist()\n",
    "    training_acc = df_acc[df_acc[\"split\"] == \"train\"][\"accuracy\"].tolist()\n",
    "    test_acc = df_acc[df_acc[\"split\"] == \"test\"][\"accuracy\"].tolist()\n",
    "\n",
    "    return state_checkpoints, steps, training_loss, test_loss, training_acc, test_acc\n",
    "\n",
    "\n",
    "# From X_test and Y_test, generate two (disjoint) datasets: one for calculating kernels and one for the remaining analysis (SVM & GP accuracy, and other metrics such as kernel alignment)\n",
    "def generate_kernel_and_analysis_datasets(\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    num_kernel_samples,\n",
    "    num_analysis_training_samples,\n",
    "    num_analysis_test_samples,\n",
    "    experiment_type,\n",
    "):\n",
    "\n",
    "    kernel_X = X_test[:num_kernel_samples]\n",
    "\n",
    "    analysis_X_train = X_test[\n",
    "        num_kernel_samples : num_kernel_samples + num_analysis_training_samples\n",
    "    ]\n",
    "    analysis_Y_train = Y_test[\n",
    "        num_kernel_samples : num_kernel_samples + num_analysis_training_samples\n",
    "    ]\n",
    "\n",
    "    analysis_X_test = X_test[\n",
    "        num_kernel_samples\n",
    "        + num_analysis_training_samples : num_kernel_samples\n",
    "        + num_analysis_training_samples\n",
    "        + num_analysis_test_samples\n",
    "    ]\n",
    "    analysis_Y_test = Y_test[\n",
    "        num_kernel_samples\n",
    "        + num_analysis_training_samples : num_kernel_samples\n",
    "        + num_analysis_training_samples\n",
    "        + num_analysis_test_samples\n",
    "    ]\n",
    "\n",
    "    if experiment_type == \"mnist\":\n",
    "        analysis_X_train = rearrange(analysis_X_train, \"b h w -> b (h w)\")\n",
    "        analysis_X_test = rearrange(analysis_X_test, \"b h w -> b (h w)\")\n",
    "\n",
    "    return (\n",
    "        kernel_X,\n",
    "        analysis_X_train,\n",
    "        analysis_Y_train,\n",
    "        analysis_X_test,\n",
    "        analysis_Y_test,\n",
    "    )\n",
    "\n",
    "\n",
    "# Return a batched kernel function where trace_axes=() [for calculating DOTS]\n",
    "def compute_kernel_trace_axes_fn(transformer_state_apply_fn):\n",
    "    kernel_fn_trace_axes = nt.empirical_kernel_fn(\n",
    "        transformer_state_apply_fn,\n",
    "        vmap_axes=0,\n",
    "        trace_axes=(),\n",
    "        implementation=nt.NtkImplementation.STRUCTURED_DERIVATIVES,\n",
    "    )\n",
    "    return kernel_fn_trace_axes\n",
    "\n",
    "\n",
    "# Return a batched kernel function where trace_axes is not defined [for computing everything other than DOTS]\n",
    "def compute_kernel_fn(transformer_state_apply_fn):\n",
    "    kernel_fn = nt.empirical_kernel_fn(\n",
    "        transformer_state_apply_fn,\n",
    "        vmap_axes=0,\n",
    "        implementation=nt.NtkImplementation.STRUCTURED_DERIVATIVES,\n",
    "    )\n",
    "    return kernel_fn\n",
    "\n",
    "\n",
    "# Apply the kernel_trace_axes_fn to the values X with given transformer_state_params\n",
    "def compute_kernel_trace_axes(\n",
    "    kernel_trace_axes_fn, transformer_state_params, X, batch_size\n",
    "):\n",
    "    kernel_trace_axes_fn_batched = nt.batch(\n",
    "        kernel_trace_axes_fn, device_count=-1, batch_size=batch_size\n",
    "    )\n",
    "    kernel_trace_axes = kernel_trace_axes_fn_batched(\n",
    "        X, None, \"ntk\", transformer_state_params\n",
    "    )\n",
    "    kernel_trace_axes = rearrange(kernel_trace_axes, \"b1 b2 d1 d2 -> (b1 d1) (b2 d2)\")\n",
    "    return kernel_trace_axes\n",
    "\n",
    "\n",
    "# Apply the kernel_fn to the values X with given transformer_state_params\n",
    "def compute_kernel(kernel_fn, transformer_state_params, X, batch_size):\n",
    "    kernel_fn_batched = nt.batch(kernel_fn, device_count=-1, batch_size=batch_size)\n",
    "    kernel = kernel_fn_batched(X, None, \"ntk\", transformer_state_params)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "# Compute DOTS on the kernel_trace_axes matrix\n",
    "def compute_dots(kernel_trace_axes):\n",
    "    kernel_rank = jax.jit(jnp.linalg.matrix_rank)(kernel_trace_axes)\n",
    "    return kernel_rank.item()\n",
    "\n",
    "\n",
    "# Create a custom_kernel_function for use in training the SVM and GP (mapping inputs X1 and X2 to a kernel matrix)\n",
    "def compute_custom_kernel_fn(kernel_fn, state_params):\n",
    "    return lambda X1, X2: kernel_fn(X1, X2, \"ntk\", state_params)\n",
    "\n",
    "\n",
    "# Given the a custom kernel and training/test data, compute the accuracy of an SVM\n",
    "def compute_svm_accuracy(\n",
    "    custom_kernel_fn,\n",
    "    analysis_X_train,\n",
    "    analysis_Y_train,\n",
    "    analysis_X_test,\n",
    "    analysis_Y_test,\n",
    "):\n",
    "    svc = SVC(kernel=custom_kernel_fn)\n",
    "    svc.fit(analysis_X_train, analysis_Y_train)\n",
    "    predictions = svc.predict(analysis_X_test)\n",
    "    accuracy = accuracy_score(analysis_Y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Given the a custom kernel and training/test data, compute the accuracy of a Gaussian Process\n",
    "def compute_gp_accuracy(\n",
    "    custom_kernel_fn,\n",
    "    analysis_X_train,\n",
    "    analysis_Y_train,\n",
    "    analysis_X_test,\n",
    "    analysis_Y_test,\n",
    "    num_y_classes,\n",
    "):\n",
    "\n",
    "    analysis_Y_train_one_hot = jax.nn.one_hot(analysis_Y_train, num_y_classes)\n",
    "\n",
    "    custom_gp_kernel = CustomKernel(kernel_fn=custom_kernel_fn)  # RBF(length_scale=1e3)\n",
    "    gaussian_process_classifier = GaussianProcessRegressor(kernel=custom_gp_kernel)\n",
    "    gaussian_process_classifier.fit(analysis_X_train, analysis_Y_train_one_hot)\n",
    "\n",
    "    predictions = gaussian_process_classifier.predict(analysis_X_test).argmax(axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(analysis_Y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Compute the kernel alignment metric (Shan 2022: A Theory of Neural Tangent Kernel Alignment and Its Influence on Training)\n",
    "def compute_kernel_alignment(kernel, analysis_Y_test):\n",
    "    kernel_alignment = (analysis_Y_test.T @ kernel @ analysis_Y_test) / (\n",
    "        jnp.linalg.norm(kernel) * jnp.linalg.norm(analysis_Y_test)\n",
    "    )\n",
    "    return kernel_alignment.item()\n",
    "\n",
    "\n",
    "# Save the computed results to the determined file. Adding kernels is controlled by the add_kernel parameter as kernels take a large space to store\n",
    "def save_results_to_json(\n",
    "    steps,\n",
    "    training_loss,\n",
    "    test_loss,\n",
    "    training_acc,\n",
    "    test_acc,\n",
    "    svm_accuracy,\n",
    "    gp_accuracy,\n",
    "    dots_results,\n",
    "    kernel_alignments,\n",
    "    computed_kernels,\n",
    "    experiment_json_file_name,\n",
    "    add_kernel,\n",
    "    logs_base_path,\n",
    "):\n",
    "\n",
    "    graph_data = {\n",
    "        \"step\": steps,\n",
    "        \"training_loss\": training_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"training_acc\": training_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"svm_accuracy\": svm_accuracy,\n",
    "        \"gp_accuracy\": gp_accuracy,\n",
    "        \"dots\": dots_results,\n",
    "        \"kernel_alignment\": kernel_alignments,\n",
    "    }\n",
    "\n",
    "    if add_kernel:\n",
    "        graph_data[\"kernels\"] = computed_kernels\n",
    "\n",
    "    json_data = json.dumps(graph_data, indent=2)\n",
    "\n",
    "    checkpoint_dir = Path(\n",
    "        logs_base_path, \"results\", f\"{experiment_json_file_name}.json\"\n",
    "    )\n",
    "\n",
    "    with open(checkpoint_dir, \"w\") as json_file:\n",
    "        json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mlogs_base_path: e.g. \"/home/dm894/idiots/logs/\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mexperiments: (experiment_name, experiment_json_file_name, experiment_checkpoint_path, experiment_type, step_distance, total_steps, num_dots_samples, num_analysis_training_samples, num_analysis_test_samples) list\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mkernel_batch_size: batch size when computing the kernels using nt.batch\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m logs_base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/dm894/idiots/logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mexperiment_name\u001b[49m\n\u001b[1;32m     20\u001b[0m experiment_json_file_name\n\u001b[1;32m     21\u001b[0m experiment_checkpoint_path\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment_name' is not defined"
     ]
    }
   ],
   "source": [
    "logs_base_path = \"/home/dm894/idiots/logs/\"\n",
    "\n",
    "experiment_name = \"mnist\"\n",
    "experiment_json_file_name = \"mnist-slower\"\n",
    "experiment_checkpoint_path = \"checkpoints/mnist-slower/checkpoints\"\n",
    "experiment_type = \"mnist\"\n",
    "step_distance = 10_000\n",
    "total_steps = 100_000\n",
    "num_kernel_samples = 256\n",
    "num_analysis_training_samples = 256\n",
    "num_analysis_test_samples = 256\n",
    "\n",
    "\n",
    "kernel_batch_size = 32\n",
    "add_kernel = False\n",
    "\n",
    "experiment_checkpoint_path = Path(logs_base_path, experiment_checkpoint_path)\n",
    "\n",
    "if experiment_type == \"algorithmic\":\n",
    "    restore_fn = algorithmic_restore\n",
    "    restore_partial_fn = algorithmic_restore_partial\n",
    "elif experiment_type == \"mnist\":\n",
    "    restore_fn = mnist_restore\n",
    "    restore_partial_fn = mnist_restore_partial\n",
    "else:\n",
    "    print(f\"Experiment type {experiment_type} not valid.\")\n",
    "    exit(1)\n",
    "\n",
    "restore_manager, _, _, ds_train, ds_test = restore_fn(experiment_checkpoint_path, 0)\n",
    "\n",
    "X_test, Y_test = jnp.array(ds_test[\"x\"]), jnp.array(ds_test[\"y\"])\n",
    "\n",
    "num_y_classes = ds_train.features[\"y\"].num_classes\n",
    "\n",
    "df = extract_data_from_checkpoints(\n",
    "    restore_manager,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    num_y_classes,\n",
    "    total_steps,\n",
    "    step_distance,\n",
    "    restore_partial_fn,\n",
    ")\n",
    "transformer_states, steps, training_loss, test_loss, training_acc, test_acc = (\n",
    "    parse_general_checkpoint_dataframe(df)\n",
    ")\n",
    "\n",
    "svm_accuracy = []\n",
    "gp_accuracy = []\n",
    "dots_results = []\n",
    "computed_kernels = []\n",
    "kernel_alignments = []\n",
    "\n",
    "# kernel dataset is used for computing the kernels used in DOTS and the remaining analysis\n",
    "# analysis datasets are used for the remaining analysis: SVM, GP, and remaining metrics such as kernel alignment\n",
    "(\n",
    "    kernel_X,\n",
    "    analysis_X_train,\n",
    "    analysis_Y_train,\n",
    "    analysis_X_test,\n",
    "    analysis_Y_test,\n",
    ") = generate_kernel_and_analysis_datasets(\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    num_kernel_samples,\n",
    "    num_analysis_training_samples,\n",
    "    num_analysis_test_samples,\n",
    "    experiment_type,\n",
    ")\n",
    "\n",
    "for i, transformer_state in enumerate(transformer_states):\n",
    "\n",
    "    gc.collect()\n",
    "    print(f\"Computing Results: {i + 1}/{len(transformer_states)}\")\n",
    "\n",
    "    kernel_trace_axes_fn = compute_kernel_trace_axes_fn(transformer_state.apply_fn)\n",
    "    kernel_fn = compute_kernel_fn(transformer_state.apply_fn)\n",
    "\n",
    "    kernel_trace_axes = compute_kernel_trace_axes(\n",
    "        kernel_trace_axes_fn,\n",
    "        transformer_state.params,\n",
    "        kernel_X,\n",
    "        kernel_batch_size,\n",
    "    )\n",
    "    kernel = compute_kernel(\n",
    "        kernel_fn, transformer_state.params, kernel_X, kernel_batch_size\n",
    "    )\n",
    "\n",
    "    custom_kernel_fn = compute_custom_kernel_fn(kernel_fn, transformer_state.params)\n",
    "\n",
    "    computed_kernels.append(kernel.tolist())\n",
    "    dots_results.append(compute_dots(kernel_trace_axes))\n",
    "    svm_accuracy.append(\n",
    "        compute_svm_accuracy(\n",
    "            custom_kernel_fn,\n",
    "            analysis_X_train,\n",
    "            analysis_Y_train,\n",
    "            analysis_X_test,\n",
    "            analysis_Y_test,\n",
    "        )\n",
    "    )\n",
    "    gp_accuracy.append(\n",
    "        compute_gp_accuracy(\n",
    "            custom_kernel_fn,\n",
    "            analysis_X_train,\n",
    "            analysis_Y_train,\n",
    "            analysis_X_test,\n",
    "            analysis_Y_test,\n",
    "            num_y_classes,\n",
    "        )\n",
    "    )\n",
    "    kernel_alignments.append(compute_kernel_alignment(kernel, analysis_Y_test))\n",
    "\n",
    "print(\"Storing Results...\")\n",
    "save_results_to_json(\n",
    "    steps,\n",
    "    training_loss,\n",
    "    test_loss,\n",
    "    training_acc,\n",
    "    test_acc,\n",
    "    svm_accuracy,\n",
    "    gp_accuracy,\n",
    "    dots_results,\n",
    "    kernel_alignments,\n",
    "    computed_kernels,\n",
    "    experiment_json_file_name,\n",
    "    add_kernel,\n",
    "    logs_base_path,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
