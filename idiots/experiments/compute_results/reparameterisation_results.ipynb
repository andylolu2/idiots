{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 14:33:46.030530: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-13 14:33:46.030582: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-13 14:33:46.032472: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-13 14:33:48.707867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "\n",
    "from idiots.dataset.dataloader import DataLoader\n",
    "from idiots.experiments.grokking.training import (\n",
    "    restore as algorithmic_restore,\n",
    "    restore_partial as algorithmic_restore_partial,\n",
    "    eval_step,\n",
    ")\n",
    "from idiots.experiments.classification.training import (\n",
    "    restore as mnist_restore,\n",
    "    restore_partial as mnist_restore_partial,\n",
    ")\n",
    "from idiots.utils import metrics\n",
    "import neural_tangents as nt\n",
    "from einops import rearrange\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import (\n",
    "    StationaryKernelMixin,\n",
    "    NormalizedKernelMixin,\n",
    "    Kernel,\n",
    "    RBF,\n",
    ")\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# GP kernel object (for compatability with sklearn.gaussian_proccess)\n",
    "class CustomKernel(StationaryKernelMixin, NormalizedKernelMixin, Kernel):\n",
    "    def __init__(self, kernel_fn):\n",
    "        self.kernel_fn = kernel_fn\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, X, Y=None, eval_gradient=False):\n",
    "        kernel = np.array(self.kernel_fn(X, Y))\n",
    "\n",
    "        if eval_gradient:\n",
    "            return kernel, np.zeros(X.shape)\n",
    "        else:\n",
    "            return kernel\n",
    "\n",
    "\n",
    "# Return the model state and training/test accuracy/loss for each timestep\n",
    "def eval_checkpoint(\n",
    "    step,\n",
    "    batch_size,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    num_classes,\n",
    "    restore_manager,\n",
    "    restore_partial_fn,\n",
    "):\n",
    "    config, state = restore_partial_fn(\n",
    "        restore_manager, step, ds_train[\"x\"][:1], num_classes\n",
    "    )\n",
    "\n",
    "    def eval_loss_acc(ds):\n",
    "        for batch in DataLoader(ds, batch_size):\n",
    "            logs = eval_step(state, batch, config.loss_variant)\n",
    "            metrics.log(**logs)\n",
    "        [losses, accuracies] = metrics.collect(\"eval_loss\", \"eval_accuracy\")\n",
    "        loss = jnp.concatenate(losses).mean().item()\n",
    "        acc = jnp.concatenate(accuracies).mean().item()\n",
    "        return loss, acc\n",
    "\n",
    "    if len(ds_train[\"x\"]) > len(ds_test[\"x\"]):\n",
    "        ds_train = ds_train.select(range(len(ds_test[\"x\"])))\n",
    "\n",
    "    train_loss, train_acc = eval_loss_acc(ds_train)\n",
    "    test_loss, test_acc = eval_loss_acc(ds_test)\n",
    "\n",
    "    return state, train_loss, train_acc, test_loss, test_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns a dataframe representing the checkpoint data of the *modeltaining: \n",
    "  - step (current checkpoint step)\n",
    "  - state (of model network)\n",
    "  - train_loss \n",
    "  - train_acc\n",
    "  - test_loss \n",
    "  - test_acc \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_data_from_checkpoints(\n",
    "    restore_manager,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    num_classes,\n",
    "    total_steps,\n",
    "    step_distance,\n",
    "    restore_partial_fn,\n",
    "    eval_checkpoint_batch_size=512,\n",
    "):\n",
    "    data = []\n",
    "    for step in range(0, total_steps, step_distance):\n",
    "\n",
    "        print(\n",
    "            f\"Loading Data: {(step // step_distance) + 1}/{total_steps // step_distance}\"\n",
    "        )\n",
    "\n",
    "        state, train_loss, train_acc, test_loss, test_acc = eval_checkpoint(\n",
    "            step,\n",
    "            eval_checkpoint_batch_size,\n",
    "            ds_train,\n",
    "            ds_test,\n",
    "            num_classes,\n",
    "            restore_manager,\n",
    "            restore_partial_fn,\n",
    "        )\n",
    "        data.append(\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"state\": state,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Parse the general checkpoint dataframe into useful sub-dataframes and lists\n",
    "def parse_general_checkpoint_dataframe(df):\n",
    "\n",
    "    state_checkpoints = df[\"state\"].tolist()\n",
    "\n",
    "    df_loss = df[[\"step\", \"train_loss\", \"test_loss\"]]\n",
    "    df_loss = df_loss.melt(\"step\", var_name=\"split\", value_name=\"loss\")\n",
    "    df_loss[\"split\"] = df_loss[\"split\"].str.replace(\"_loss\", \"\")\n",
    "\n",
    "    df_acc = df[[\"step\", \"train_acc\", \"test_acc\"]]\n",
    "    df_acc = df_acc.melt(\"step\", var_name=\"split\", value_name=\"accuracy\")\n",
    "    df_acc[\"split\"] = df_acc[\"split\"].str.replace(\"_acc\", \"\")\n",
    "\n",
    "    steps = df[\"step\"].tolist()\n",
    "    training_loss = df_loss[df_loss[\"split\"] == \"train\"][\"loss\"].tolist()\n",
    "    test_loss = df_loss[df_loss[\"split\"] == \"test\"][\"loss\"].tolist()\n",
    "    training_acc = df_acc[df_acc[\"split\"] == \"train\"][\"accuracy\"].tolist()\n",
    "    test_acc = df_acc[df_acc[\"split\"] == \"test\"][\"accuracy\"].tolist()\n",
    "\n",
    "    return state_checkpoints, steps, training_loss, test_loss, training_acc, test_acc\n",
    "\n",
    "\n",
    "# From X_test and Y_test, generate two (disjoint) datasets: one for calculating kernels and one for the remaining analysis (SVM & GP accuracy, and other metrics such as kernel alignment)\n",
    "def generate_kernel_and_analysis_datasets(\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    num_kernel_samples,\n",
    "    num_analysis_training_samples,\n",
    "    num_analysis_test_samples,\n",
    "    experiment_type,\n",
    "):\n",
    "\n",
    "    kernel_X = X_test[:num_kernel_samples]\n",
    "\n",
    "    analysis_X_train = X_test[\n",
    "        num_kernel_samples : num_kernel_samples + num_analysis_training_samples\n",
    "    ]\n",
    "    analysis_Y_train = Y_test[\n",
    "        num_kernel_samples : num_kernel_samples + num_analysis_training_samples\n",
    "    ]\n",
    "\n",
    "    analysis_X_test = X_test[\n",
    "        num_kernel_samples\n",
    "        + num_analysis_training_samples : num_kernel_samples\n",
    "        + num_analysis_training_samples\n",
    "        + num_analysis_test_samples\n",
    "    ]\n",
    "    analysis_Y_test = Y_test[\n",
    "        num_kernel_samples\n",
    "        + num_analysis_training_samples : num_kernel_samples\n",
    "        + num_analysis_training_samples\n",
    "        + num_analysis_test_samples\n",
    "    ]\n",
    "\n",
    "    if experiment_type == \"mnist\":\n",
    "        analysis_X_train = rearrange(analysis_X_train, \"b h w -> b (h w)\")\n",
    "        analysis_X_test = rearrange(analysis_X_test, \"b h w -> b (h w)\")\n",
    "\n",
    "    return (\n",
    "        kernel_X,\n",
    "        analysis_X_train,\n",
    "        analysis_Y_train,\n",
    "        analysis_X_test,\n",
    "        analysis_Y_test,\n",
    "    )\n",
    "\n",
    "\n",
    "# Return a batched kernel function where trace_axes=() [for calculating DOTS]\n",
    "def compute_kernel_trace_axes_fn(model_state_apply_fn):\n",
    "    kernel_fn_trace_axes = nt.empirical_kernel_fn(\n",
    "        model_state_apply_fn,\n",
    "        vmap_axes=0,\n",
    "        trace_axes=(),\n",
    "        implementation=nt.NtkImplementation.STRUCTURED_DERIVATIVES,\n",
    "    )\n",
    "    return kernel_fn_trace_axes\n",
    "\n",
    "\n",
    "# Return a batched kernel function where trace_axes is not defined [for computing everything other than DOTS]\n",
    "def compute_kernel_fn(model_state_apply_fn):\n",
    "    kernel_fn = nt.empirical_kernel_fn(\n",
    "        model_state_apply_fn,\n",
    "        vmap_axes=0,\n",
    "        implementation=nt.NtkImplementation.STRUCTURED_DERIVATIVES,\n",
    "    )\n",
    "    return kernel_fn\n",
    "\n",
    "\n",
    "# Apply the kernel_trace_axes_fn to the values X with given model_state_params\n",
    "def compute_kernel_trace_axes(\n",
    "    kernel_trace_axes_fn, model_state_params, X, batch_size\n",
    "):\n",
    "    kernel_trace_axes_fn_batched = nt.batch(\n",
    "        kernel_trace_axes_fn, device_count=-1, batch_size=batch_size\n",
    "    )\n",
    "    kernel_trace_axes = kernel_trace_axes_fn_batched(\n",
    "        X, None, \"ntk\", model_state_params\n",
    "    )\n",
    "    kernel_trace_axes = rearrange(kernel_trace_axes, \"b1 b2 d1 d2 -> (b1 d1) (b2 d2)\")\n",
    "    return kernel_trace_axes\n",
    "\n",
    "\n",
    "# Apply the kernel_fn to the values X with given model_state_params\n",
    "def compute_kernel(kernel_fn, model_state_params, X, batch_size):\n",
    "    kernel_fn_batched = nt.batch(kernel_fn, device_count=-1, batch_size=batch_size)\n",
    "    kernel = kernel_fn_batched(X, None, \"ntk\", model_state_params)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "# Compute DOTS on the kernel_trace_axes matrix\n",
    "def compute_dots(kernel_trace_axes):\n",
    "    kernel_rank = jax.jit(jnp.linalg.matrix_rank)(kernel_trace_axes)\n",
    "    return kernel_rank.item()\n",
    "\n",
    "\n",
    "# Create a custom_kernel_function for use in training the SVM and GP (mapping inputs X1 and X2 to a kernel matrix)\n",
    "def compute_custom_kernel_fn(kernel_fn, state_params):\n",
    "    return lambda X1, X2: kernel_fn(X1, X2, \"ntk\", state_params)\n",
    "\n",
    "\n",
    "# Given the a custom kernel and training/test data, compute the accuracy of an SVM\n",
    "def compute_svm_accuracy(\n",
    "    custom_kernel_fn,\n",
    "    analysis_X_train,\n",
    "    analysis_Y_train,\n",
    "    analysis_X_test,\n",
    "    analysis_Y_test,\n",
    "):\n",
    "    svc = SVC(kernel=custom_kernel_fn)\n",
    "    svc.fit(analysis_X_train, analysis_Y_train)\n",
    "    predictions = svc.predict(analysis_X_test)\n",
    "    accuracy = accuracy_score(analysis_Y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Given the a custom kernel and training/test data, compute the accuracy of a Gaussian Process\n",
    "def compute_gp_accuracy(\n",
    "    custom_kernel_fn,\n",
    "    analysis_X_train,\n",
    "    analysis_Y_train,\n",
    "    analysis_X_test,\n",
    "    analysis_Y_test,\n",
    "    num_y_classes,\n",
    "):\n",
    "\n",
    "    analysis_Y_train_one_hot = jax.nn.one_hot(analysis_Y_train, num_y_classes)\n",
    "\n",
    "    custom_gp_kernel = CustomKernel(kernel_fn=custom_kernel_fn)  # RBF(length_scale=1e3)\n",
    "    gaussian_process_classifier = GaussianProcessRegressor(kernel=custom_gp_kernel)\n",
    "    gaussian_process_classifier.fit(analysis_X_train, analysis_Y_train_one_hot)\n",
    "\n",
    "    predictions = gaussian_process_classifier.predict(analysis_X_test).argmax(axis=-1)\n",
    "\n",
    "    accuracy = accuracy_score(analysis_Y_test, predictions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Compute the kernel alignment metric (Shan 2022: A Theory of Neural Tangent Kernel Alignment and Its Influence on Training)\n",
    "def compute_kernel_alignment(kernel, analysis_Y_test):\n",
    "    kernel_alignment = (analysis_Y_test.T @ kernel @ analysis_Y_test) / (\n",
    "        jnp.linalg.norm(kernel) * jnp.linalg.norm(analysis_Y_test)\n",
    "    )\n",
    "    return kernel_alignment.item()\n",
    "\n",
    "\n",
    "# Save the computed results to the determined file. Adding kernels is controlled by the add_kernel parameter as kernels take a large space to store\n",
    "def save_results_to_json(\n",
    "    steps,\n",
    "    training_loss,\n",
    "    test_loss,\n",
    "    training_acc,\n",
    "    test_acc,\n",
    "    svm_accuracy,\n",
    "    gp_accuracy,\n",
    "    dots_results,\n",
    "    kernel_alignments,\n",
    "    computed_kernels,\n",
    "    experiment_json_file_name,\n",
    "    add_kernel,\n",
    "    logs_base_path,\n",
    "):\n",
    "\n",
    "    graph_data = {\n",
    "        \"step\": steps,\n",
    "        \"training_loss\": training_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"training_acc\": training_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"svm_accuracy\": svm_accuracy,\n",
    "        \"gp_accuracy\": gp_accuracy,\n",
    "        \"dots\": dots_results,\n",
    "        \"kernel_alignment\": kernel_alignments,\n",
    "    }\n",
    "\n",
    "    if add_kernel:\n",
    "        graph_data[\"kernels\"] = computed_kernels\n",
    "\n",
    "    json_data = json.dumps(graph_data, indent=2)\n",
    "\n",
    "    checkpoint_dir = Path(\n",
    "        logs_base_path, \"results\", f\"{experiment_json_file_name}.json\"\n",
    "    )\n",
    "\n",
    "    with open(checkpoint_dir, \"w\") as json_file:\n",
    "        json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Given directory is read only=/home/dm894/idiots/logs/checkpoints/mnist-slower/checkpoints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data: 1/10\n",
      "Loading Data: 2/10\n",
      "Loading Data: 3/10\n",
      "Loading Data: 4/10\n",
      "Loading Data: 5/10\n",
      "Loading Data: 6/10\n",
      "Loading Data: 7/10\n",
      "Loading Data: 8/10\n",
      "Loading Data: 9/10\n",
      "Loading Data: 10/10\n"
     ]
    }
   ],
   "source": [
    "logs_base_path = \"/home/dm894/idiots/logs/\"\n",
    "\n",
    "experiment_name = \"mnist\"\n",
    "experiment_json_file_name = \"mnist-slower\"\n",
    "experiment_checkpoint_path = \"checkpoints/mnist-slower/checkpoints\"\n",
    "experiment_type = \"mnist\"\n",
    "step_distance = 10_000\n",
    "total_steps = 100_000\n",
    "num_kernel_samples = 128\n",
    "num_analysis_training_samples = 128\n",
    "num_analysis_test_samples = 128\n",
    "\n",
    "\n",
    "kernel_batch_size = 32\n",
    "add_kernel = False\n",
    "\n",
    "experiment_checkpoint_path = Path(logs_base_path, experiment_checkpoint_path)\n",
    "\n",
    "if experiment_type == \"algorithmic\":\n",
    "    restore_fn = algorithmic_restore\n",
    "    restore_partial_fn = algorithmic_restore_partial\n",
    "elif experiment_type == \"mnist\":\n",
    "    restore_fn = mnist_restore\n",
    "    restore_partial_fn = mnist_restore_partial\n",
    "else:\n",
    "    print(f\"Experiment type {experiment_type} not valid.\")\n",
    "    exit(1)\n",
    "\n",
    "restore_manager, _, _, ds_train, ds_test = restore_fn(experiment_checkpoint_path, 0)\n",
    "\n",
    "X_test, Y_test = jnp.array(ds_test[\"x\"]), jnp.array(ds_test[\"y\"])\n",
    "\n",
    "num_y_classes = ds_train.features[\"y\"].num_classes\n",
    "\n",
    "df = extract_data_from_checkpoints(\n",
    "    restore_manager,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    num_y_classes,\n",
    "    total_steps,\n",
    "    step_distance,\n",
    "    restore_partial_fn,\n",
    ")\n",
    "model_states, steps, training_loss, test_loss, training_acc, test_acc = (\n",
    "    parse_general_checkpoint_dataframe(df)\n",
    ")\n",
    "\n",
    "svm_accuracy = []\n",
    "gp_accuracy = []\n",
    "dots_results = []\n",
    "computed_kernels = []\n",
    "kernel_alignments = []\n",
    "\n",
    "# kernel dataset is used for computing the kernels used in DOTS and the remaining analysis\n",
    "# analysis datasets are used for the remaining analysis: SVM, GP, and remaining metrics such as kernel alignment\n",
    "(\n",
    "    kernel_X,\n",
    "    analysis_X_train,\n",
    "    analysis_Y_train,\n",
    "    analysis_X_test,\n",
    "    analysis_Y_test,\n",
    ") = generate_kernel_and_analysis_datasets(\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    num_kernel_samples,\n",
    "    num_analysis_training_samples,\n",
    "    num_analysis_test_samples,\n",
    "    experiment_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Results: 1/10\n",
      "Computing Results: 2/10\n",
      "Computing Results: 3/10\n",
      "Computing Results: 4/10\n",
      "Computing Results: 5/10\n",
      "Computing Results: 6/10\n",
      "Computing Results: 7/10\n",
      "Computing Results: 8/10\n",
      "Computing Results: 9/10\n",
      "Computing Results: 10/10\n",
      "Storing Results...\n"
     ]
    }
   ],
   "source": [
    "kernel_trace_axes_fn = compute_kernel_trace_axes_fn(model_states[0].apply_fn)\n",
    "kernel_fn = compute_kernel_fn(model_states[0].apply_fn)\n",
    "\n",
    "for i, model_state in enumerate(model_states):\n",
    "\n",
    "    gc.collect()\n",
    "    print(f\"Computing Results: {i + 1}/{len(model_states)}\")\n",
    "\n",
    "    kernel_trace_axes = compute_kernel_trace_axes(\n",
    "        kernel_trace_axes_fn,\n",
    "        model_state.params,\n",
    "        kernel_X,\n",
    "        kernel_batch_size,\n",
    "    )\n",
    "    kernel = compute_kernel(\n",
    "        kernel_fn, model_state.params, kernel_X, kernel_batch_size\n",
    "    )\n",
    "\n",
    "    custom_kernel_fn = compute_custom_kernel_fn(kernel_fn, model_state.params)\n",
    "\n",
    "    computed_kernels.append(kernel.tolist())\n",
    "    dots_results.append(compute_dots(kernel_trace_axes))\n",
    "    svm_accuracy.append(\n",
    "        compute_svm_accuracy(\n",
    "            custom_kernel_fn,\n",
    "            analysis_X_train,\n",
    "            analysis_Y_train,\n",
    "            analysis_X_test,\n",
    "            analysis_Y_test,\n",
    "        )\n",
    "    )\n",
    "    gp_accuracy.append(\n",
    "        compute_gp_accuracy(\n",
    "            custom_kernel_fn,\n",
    "            analysis_X_train,\n",
    "            analysis_Y_train,\n",
    "            analysis_X_test,\n",
    "            analysis_Y_test,\n",
    "            num_y_classes,\n",
    "        )\n",
    "    )\n",
    "    kernel_alignments.append(compute_kernel_alignment(kernel, analysis_Y_test))\n",
    "\n",
    "print(\"Storing Results...\")\n",
    "save_results_to_json(\n",
    "    steps,\n",
    "    training_loss,\n",
    "    test_loss,\n",
    "    training_acc,\n",
    "    test_acc,\n",
    "    svm_accuracy,\n",
    "    gp_accuracy,\n",
    "    dots_results,\n",
    "    kernel_alignments,\n",
    "    computed_kernels,\n",
    "    experiment_json_file_name,\n",
    "    add_kernel,\n",
    "    logs_base_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Reparameterisation (reparam=2): 1/4\n",
      "Computing Reparameterisation (reparam=10): 2/4\n",
      "Computing Reparameterisation (reparam=100): 3/4\n",
      "Computing Reparameterisation (reparam=1000): 4/4\n"
     ]
    }
   ],
   "source": [
    "from idiots.experiments.grokking.training import eval_step\n",
    "from copy import deepcopy\n",
    "\n",
    "kernel_fn = compute_kernel_fn(model_states[0].apply_fn)\n",
    "kernel_trace_axes_fn = compute_kernel_trace_axes_fn(model_states[0].apply_fn)\n",
    "\n",
    "def compute_reparam_model_accuracy(model_state, batch_size=32):\n",
    "  batch_accuracy = []\n",
    "\n",
    "  for batch in DataLoader(ds_test, batch_size):\n",
    "      logs = eval_step(model_state, batch, \"cross_entropy\")\n",
    "      batch_accuracy.append(logs[\"eval_accuracy\"])\n",
    "\n",
    "  return jnp.concatenate(batch_accuracy).mean().item()\n",
    "\n",
    "def compute_reparam_svm_accuracy(model_state):\n",
    "\n",
    "  custom_kernel_fn = compute_custom_kernel_fn(kernel_fn, model_state.params)\n",
    "\n",
    "  return compute_svm_accuracy(\n",
    "            custom_kernel_fn,\n",
    "            analysis_X_train,\n",
    "            analysis_Y_train,\n",
    "            analysis_X_test,\n",
    "            analysis_Y_test,\n",
    "        )\n",
    "  \n",
    "def compute_reparam_dots(model_state): \n",
    "\n",
    "  kernel_trace_axes = compute_kernel_trace_axes(\n",
    "          kernel_trace_axes_fn,\n",
    "          model_state.params,\n",
    "          kernel_X,\n",
    "          kernel_batch_size,\n",
    "      )\n",
    "  \n",
    "  return compute_dots(kernel_trace_axes)\n",
    "\n",
    "reparam_list = [2, 10, 100, 1000, 10_000, 100_000, 1_000_000]\n",
    "\n",
    "for i, reparam in enumerate(reparam_list):\n",
    "\n",
    "  gc.collect()\n",
    "  print(f\"Computing Reparameterisation (reparam={reparam}): {i + 1}/{len(reparam_list)}\")\n",
    "\n",
    "  reparam_model_accuracy_history = [] \n",
    "  reparam_svm_accuracy_history = [] \n",
    "  reparam_dots_history = [] \n",
    "\n",
    "  for model_state in model_states: \n",
    "\n",
    "    modified_model_state = deepcopy(model_state)\n",
    "    modified_model_state.params[\"params\"][\"Dense_0\"][\"kernel\"] = reparam * modified_model_state.params[\"params\"][\"Dense_0\"][\"kernel\"]\n",
    "    modified_model_state.params[\"params\"][\"Dense_0\"][\"bias\"] = reparam * modified_model_state.params[\"params\"][\"Dense_0\"][\"bias\"]\n",
    "    modified_model_state.params[\"params\"][\"Dense_1\"][\"kernel\"] = (1 / reparam) * modified_model_state.params[\"params\"][\"Dense_1\"][\"kernel\"]\n",
    "\n",
    "    reparam_model_accuracy_history.append(compute_reparam_model_accuracy(modified_model_state))\n",
    "    reparam_svm_accuracy_history.append(compute_reparam_svm_accuracy(modified_model_state))\n",
    "    reparam_dots_history.append(compute_reparam_dots(modified_model_state))\n",
    "\n",
    "  reparam_graph_data = {\n",
    "    \"reparam_test_acc\": reparam_model_accuracy_history,\n",
    "    \"reparam_svm_accuracy\": reparam_svm_accuracy_history,\n",
    "    \"reparam_dots\": reparam_dots_history\n",
    "  }\n",
    "\n",
    "  json_data = json.dumps(reparam_graph_data, indent=2)\n",
    "\n",
    "  checkpoint_dir = Path(\n",
    "          logs_base_path, \"results\", f\"{experiment_json_file_name}-reparam-{reparam}.json\"\n",
    "      )\n",
    "\n",
    "  with open(checkpoint_dir, \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
